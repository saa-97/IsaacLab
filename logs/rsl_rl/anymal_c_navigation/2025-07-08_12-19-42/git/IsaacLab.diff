--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/__init__.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/__init__.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/__init__.py
index dd4adfb1..d8313471 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/__init__.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/__init__.py
@@ -57,3 +57,26 @@ gym.register(
         "skrl_cfg_entry_point": f"{agents.__name__}:skrl_flat_ppo_cfg.yaml",
     },
 )
+
+gym.register(
+    id="Isaac-Navigation-H1-v0",
+    entry_point="isaaclab.envs:ManagerBasedRLEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.blind_navigation:H1RoughEnvCfg",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:H1FlatPPORunnerCfg",
+        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_flat_ppo_cfg.yaml",
+    },
+)
+
+
+gym.register(
+    id="Isaac-Navigation-H1-Play-v0",
+    entry_point="isaaclab.envs:ManagerBasedRLEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.blind_navigation:H1RoughEnvCfg_PLAY",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:H1FlatPPORunnerCfg",
+        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_flat_ppo_cfg.yaml",
+    },
+)
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
index a0d6a79a..72f4bd82 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
@@ -1,26 +1,107 @@
 #Rough_env_cfg from Manager Based RL, Config H1.
 
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
+from __future__ import annotations
+
+from isaaclab.managers import RewardTermCfg as RewTerm
+from isaaclab.managers import SceneEntityCfg
+from isaaclab.utils import configclass
+from isaaclab.managers import CurriculumTermCfg as CurrTerm
 import torch
-from omni.isaac.lab.managers import CommandTermCfg
+from isaaclab.managers import CommandTermCfg
+from isaaclab.managers import CurriculumTermCfg as CurrTerm
+from isaaclab.managers import EventTermCfg as EventTerm
+from isaaclab.managers import ObservationGroupCfg as ObsGroup
+from isaaclab.managers import ObservationTermCfg as ObsTerm
+from isaaclab.utils.noise import AdditiveUniformNoiseCfg as Unoise
 # source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py
 
-import omni.isaac.lab_tasks.manager_based.navigation.mdp as mdp
-import omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg, RewardsCfg
+import isaaclab_tasks.manager_based.navigation.mdp as mdpa
+import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp
+from isaaclab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg, RewardsCfg
 # from omni.isaac.lab.managers import TimedataCfg
 ##
 # Pre-defined configs
 ##
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
+from isaaclab_assets import H1_MINIMAL_CFG  # isort: skip
 
 import math
 
 
 
+@configclass
+class ObservationsCfg:
+    """Observation specifications for the MDP."""
+
+    @configclass
+    class PolicyCfg(ObsGroup):
+        """Observations for policy group."""
+        pose_command = ObsTerm(func=mdp.generated_commands, params={"command_name": "pose_command"})
+
+        # observation terms (order preserved)
+        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
+        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
+        projected_gravity = ObsTerm(
+            func=mdp.projected_gravity,
+            noise=Unoise(n_min=-0.05, n_max=0.05),
+        )
+        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
+        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
+        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
+        actions = ObsTerm(func=mdp.last_action)
+        height_scan = ObsTerm(
+            func=mdp.height_scan,
+            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
+            noise=Unoise(n_min=-0.1, n_max=0.1),
+            clip=(-1.0, 1.0),
+        )
+
+#added code
+        time_left = ObsTerm(
+            func=lambda env: (
+                env.command_manager.get_term("pose_command").time_left / 
+                env.command_manager.get_term("pose_command").cfg.resampling_time_range[1]
+            ).unsqueeze(-1),
+            scale=1.0,
+            clip=(0.0, 1.0)
+        )
+
+        target_position = ObsTerm(
+            func=lambda env: (
+                (env.command_manager.get_term("pose_command").pos_command_w[:, :2] - 
+                env.scene["robot"].data.root_link_pos_w[:, :2]).view(-1, 2)  # Ensure shape (num_envs, 2)
+            ),
+            noise=Unoise(n_min=-0.1, n_max=0.1),
+            scale=0.2,
+            clip=(-1.0, 1.0)
+        )
+
+        def __post_init__(self):
+            self.enable_corruption = True
+            self.concatenate_terms = True
+
+    # observation groups
+    policy: PolicyCfg = PolicyCfg()
+
+
+@configclass
+class CommandsCfg:
+    """Command specifications for the MDP."""
+
+
+
+    pose_command = mdp.UniformPose2dCommandCfg(
+        asset_name="robot",
+        simple_heading=False,
+        resampling_time_range=(8, 8),
+        debug_vis=True,
+        # Generate a tensor with [pos_x, pos_y, heading]
+        ranges=mdp.UniformPose2dCommandCfg.Ranges(
+            pos_x=(-3.0, 3.0),
+            pos_y=(-3.0, 3.0),
+            heading=(-math.pi, math.pi)
+        ),
+    )
+
 
 @configclass
 class H1Rewards(RewardsCfg):
@@ -31,21 +112,21 @@ class H1Rewards(RewardsCfg):
 
 #code not being used now
 
-    position_tracking = RewTerm(
-        func=mdp.position_command_error_tanh,
-        weight=0.5,
-        params={"std": 4, "command_name": "pose_command"},
-    )
-    position_tracking_fine_grained = RewTerm(
-        func=mdp.position_command_error_tanh,
-        weight=0.5,
-        params={"std": 0.4, "command_name": "pose_command"},
-    )
-    orientation_tracking = RewTerm(
-        func=mdp.heading_command_error_abs,
-        weight=-0.2,
-        params={"command_name": "pose_command"},
-    )
+    # position_tracking = RewTerm(
+    #     func=mdpa.position_command_error_tanh,
+    #     weight=0.5,
+    #     params={"std": 4, "command_name": "pose_command"},
+    # )
+    # position_tracking_fine_grained = RewTerm(
+    #     func=mdpa.position_command_error_tanh,
+    #     weight=0.5,
+    #     params={"std": 0.4, "command_name": "pose_command"},
+    # )
+    # orientation_tracking = RewTerm(
+    #     func=mdpa.heading_command_error_abs,
+    #     weight=-0.2,
+    #     params={"command_name": "pose_command"},
+    # )
 
 
 #navigation related code from blind_3 paper
@@ -215,27 +296,28 @@ class H1RoughEnvCfg_PLAY(H1RoughEnvCfg):
 import math
 import torch
 from dataclasses import MISSING
-from omni.isaac.lab.utils.math import quat_rotate_inverse
-import omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp as mdp
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
+from isaaclab.utils.math import quat_rotate_inverse
+import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp
+import isaaclab.sim as sim_utils
+from isaaclab.assets import ArticulationCfg, AssetBaseCfg
+from isaaclab.envs import ManagerBasedRLEnvCfg
+from isaaclab.managers import CurriculumTermCfg as CurrTerm
+from isaaclab.managers import EventTermCfg as EventTerm
+from isaaclab.managers import ObservationGroupCfg as ObsGroup
+from isaaclab.managers import ObservationTermCfg as ObsTerm
+from isaaclab.managers import RewardTermCfg as RewTerm
+from isaaclab.managers import SceneEntityCfg
+from isaaclab.managers import TerminationTermCfg as DoneTerm
+from isaaclab.scene import InteractiveSceneCfg
+from isaaclab.sensors import ContactSensorCfg, RayCasterCfg, patterns
+from isaaclab.terrains import TerrainImporterCfg
+from isaaclab.utils import configclass
+from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
+from isaaclab.utils.noise import AdditiveUniformNoiseCfg as Unoise
 # from omni.isaac.lab.terrains import TerrainImporterCfg  # Uncomment this line
-import omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp as mdp
-from omni.isaac.lab_assets import H1_MINIMAL_CFG
-from omni.isaac.lab.assets import RigidObjectCfg  # Add this import
+import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp
+from isaaclab_assets import H1_MINIMAL_CFG  # isort: skip
+from isaaclab.assets import RigidObjectCfg  # Add this import
 
 # from omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp import UniformVelocityCommand
 
@@ -567,16 +649,14 @@ The functions can be passed to the :class:`omni.isaac.lab.managers.RewardTermCfg
 specify the reward function and its parameters.
 """
 
-from __future__ import annotations
 
-from __future__ import annotations
 
 import torch
 from typing import TYPE_CHECKING
 
-from omni.isaac.lab.assets import RigidObject
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.utils.math import combine_frame_transforms, quat_error_magnitude, quat_mul
+from isaaclab.assets import RigidObject
+from isaaclab.managers import SceneEntityCfg
+from isaaclab.utils.math import combine_frame_transforms, quat_error_magnitude, quat_mul
 
 if TYPE_CHECKING:
     from isaaclab.envs import ManagerBasedRLEnv
@@ -586,17 +666,17 @@ import torch
 from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
-    from omni.isaac.lab.envs import ManagerBasedRLEnv
+    from isaaclab.envs import ManagerBasedRLEnv
 
 import torch
 from typing import TYPE_CHECKING
 
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.sensors import ContactSensor
-from omni.isaac.lab.utils.math import quat_rotate_inverse, yaw_quat
+from isaaclab.managers import SceneEntityCfg
+from isaaclab.sensors import ContactSensor
+from isaaclab.utils.math import quat_rotate_inverse, yaw_quat
 
 if TYPE_CHECKING:
-    from omni.isaac.lab.envs import ManagerBasedRLEnv
+    from isaaclab.envs import ManagerBasedRLEnv
 
 
 def feet_air_time(
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
index 7a1fc12a..1e0786f6 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
@@ -14,6 +14,11 @@ from __future__ import annotations
 import torch
 from typing import TYPE_CHECKING
 
+from isaaclab.assets import RigidObject
+from isaaclab.managers import SceneEntityCfg
+from isaaclab.utils.math import combine_frame_transforms, quat_error_magnitude, quat_mul
+
+
 from isaaclab.envs import mdp
 from isaaclab.managers import SceneEntityCfg
 from isaaclab.sensors import ContactSensor
@@ -106,6 +111,136 @@ def track_ang_vel_z_world_exp(
     ang_vel_error = torch.square(env.command_manager.get_command(command_name)[:, 2] - asset.data.root_ang_vel_w[:, 2])
     return torch.exp(-ang_vel_error / std**2)
 
+def position_command_error_tanh(env: ManagerBasedRLEnv, std: float, command_name: str) -> torch.Tensor:
+    """Reward position tracking with tanh kernel."""
+    command = env.command_manager.get_command(command_name)
+    des_pos_b = command[:, :3]
+    distance = torch.norm(des_pos_b, dim=1)
+    return 1 - torch.tanh(distance / std)
+
+def heading_command_error_abs(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
+    """Penalize tracking orientation error."""
+    command = env.command_manager.get_command(command_name)
+    heading_b = command[:, 3]
+    return heading_b.abs()
+
+def position_command_error(env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg) -> torch.Tensor:
+    """Penalize tracking of the position error using L2-norm.
+
+    The function computes the position error between the desired position (from the command) and the
+    current position of the asset's body (in world frame). The position error is computed as the L2-norm
+    of the difference between the desired and current positions.
+    """
+    # extract the asset (to enable type hinting)
+    asset: RigidObject = env.scene[asset_cfg.name]
+    command = env.command_manager.get_command(command_name)
+    # obtain the desired and current positions
+    des_pos_b = command[:, :3]
+    des_pos_w, _ = combine_frame_transforms(asset.data.root_state_w[:, :3], asset.data.root_state_w[:, 3:7], des_pos_b)
+    curr_pos_w = asset.data.body_state_w[:, asset_cfg.body_ids[0], :3]  # type: ignore
+    return torch.norm(curr_pos_w - des_pos_w, dim=1)
+
+def position_command_error_tanh(
+    env: ManagerBasedRLEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg
+) -> torch.Tensor:
+    """Reward tracking of the position using the tanh kernel.
+
+    The function computes the position error between the desired position (from the command) and the
+    current position of the asset's body (in world frame) and maps it with a tanh kernel.
+    """
+    # extract the asset (to enable type hinting)
+    asset: RigidObject = env.scene[asset_cfg.name]
+    command = env.command_manager.get_command(command_name)
+    # obtain the desired and current positions
+    des_pos_b = command[:, :3]
+    des_pos_w, _ = combine_frame_transforms(asset.data.root_state_w[:, :3], asset.data.root_state_w[:, 3:7], des_pos_b)
+    curr_pos_w = asset.data.body_state_w[:, asset_cfg.body_ids[0], :3]  # type: ignore
+    distance = torch.norm(curr_pos_w - des_pos_w, dim=1)
+    return 1 - torch.tanh(distance / std)
+
+def orientation_command_error(env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg) -> torch.Tensor:
+    """Penalize tracking orientation error using shortest path.
+
+    The function computes the orientation error between the desired orientation (from the command) and the
+    current orientation of the asset's body (in world frame). The orientation error is computed as the shortest
+    path between the desired and current orientations.
+    """
+    # extract the asset (to enable type hinting)
+    asset: RigidObject = env.scene[asset_cfg.name]
+    command = env.command_manager.get_command(command_name)
+    # obtain the desired and current orientations
+    des_quat_b = command[:, 3:7]
+    des_quat_w = quat_mul(asset.data.root_state_w[:, 3:7], des_quat_b)
+    curr_quat_w = asset.data.body_state_w[:, asset_cfg.body_ids[0], 3:7]  # type: ignore
+    return quat_error_magnitude(curr_quat_w, des_quat_w)
+
+def compute_terminal_reward(env, Tr=1.0):
+
+    time_left = env.command_manager.get_term("pose_command").time_left  # e.g., [N, M]
+    active = (time_left < Tr).float()  # still [N, M]
+    
+    xb = env.scene["robot"].data.root_link_pos_w[:, :3]  # [N, 3]
+    target = env.command_manager.get_term("pose_command").pos_command_w[:, :3]  # [N, 3] or [N, M, 3]
+    
+    if target.dim() == 2:
+        target = target.unsqueeze(1)  # becomes [N, 1, 3]
+    xb = xb.unsqueeze(1)  # becomes [N, 1, 3]
+    
+    dist2 = torch.sum((xb - target)**2, dim=-1)
+    
+    reward = active * (1.0 / Tr) * (1.0 / (1.0 + dist2))
+    return reward.mean(dim=1)
+
+def compute_stalling_penalty(env):
+    """
+    Computes a penalty for stalling far from the target, matching paper's r_stall.
+    """
+    v = torch.norm(env.scene["robot"].data.root_com_lin_vel_w[:, :3], dim=1, keepdim=True)
+    
+    xb = env.scene["robot"].data.root_link_pos_w[:, :3].unsqueeze(1)  # [N, 1, 3]
+    target = env.command_manager.get_term("pose_command").pos_command_w[:, :3]
+    if target.dim() == 2:
+        target = target.unsqueeze(1)  # [N, 1, 3]
+    dist = torch.norm(xb - target, dim=-1)
+    
+    condition = ((v < 0.1) & (dist > 0.5)).float()  # [N, 1]
+    penalty = -1.0 * condition
+    return penalty.mean(dim=1)
+
+
+
+def compute_exploration_reward(env, task_reward_tracker=0.0, is_terminal=False, remove_threshold=0.5):
+
+    v = env.scene["robot"].data.root_com_lin_vel_w[:, :3]    # shape [N, 3]
+    x = env.scene["robot"].data.root_link_pos_w[:, :3]       # shape [N, 3]
+    x_star = env.command_manager.get_term("pose_command").pos_command_w[:, :3]  # shape [N, 3]
+    
+    if x_star.dim() == 3:  
+        x_star = x_star.squeeze(1)  # now shape [N, 3]
+    
+    dot_prod = torch.sum(v * (x_star - x), dim=1)  # shape [N]
+    norm_v = torch.norm(v, dim=1) + 1e-6           # shape [N]
+    norm_diff = torch.norm(x_star - x, dim=1) + 1e-6  # shape [N]
+    r_bias = dot_prod / (norm_v * norm_diff)        # shape [N]
+    
+
+    if isinstance(task_reward_tracker, (int, float)):
+        task_reward_tracker = torch.tensor(task_reward_tracker, device=v.device, dtype=torch.float32).repeat(v.shape[0])
+    mask = (task_reward_tracker < remove_threshold).float()  # shape [N]
+    
+    return r_bias * mask  # shape [N]
+
+
+def compute_stop_reward(env, dist_threshold=0.2, vel_threshold=0.05, stop_penalty=-10.0):
+
+    x = env.scene["robot"].data.root_link_pos_w[:, :3]  # [N, 3]
+    target = env.command_manager.get_term("pose_command").pos_command_w[:, :3]  # [N, 3]
+    dist = torch.norm(x - target, dim=1)  # [N]
+    
+    vel = torch.norm(env.scene["robot"].data.root_com_lin_vel_w[:, :3], dim=1)  # [N]
+    
+    penalty = stop_penalty * ((dist < dist_threshold).float() * (vel > vel_threshold).float())
+    return penalty  # [N]
 
 def stand_still_joint_deviation_l1(
     env, command_name: str, command_threshold: float = 0.06, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py
index 2f2162fd..53860e87 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py
@@ -4,9 +4,11 @@
 # SPDX-License-Identifier: BSD-3-Clause
 
 import math
-
+from isaaclab.assets import RigidObjectCfg
+from isaaclab.sim.schemas.schemas_cfg import RigidBodyPropertiesCfg
 from isaaclab.envs import ManagerBasedRLEnvCfg
 from isaaclab.managers import EventTermCfg as EventTerm
+from isaaclab.sim.spawners.from_files.from_files_cfg import UsdFileCfg
 from isaaclab.managers import ObservationGroupCfg as ObsGroup
 from isaaclab.managers import ObservationTermCfg as ObsTerm
 from isaaclab.managers import RewardTermCfg as RewTerm
@@ -152,7 +154,27 @@ class NavigationEnvCfg_PLAY(NavigationEnvCfg):
     def __post_init__(self) -> None:
         # post init of parent
         super().__post_init__()
-
+        # Rigid body properties of each cube
+        cube_properties = RigidBodyPropertiesCfg(
+            solver_position_iteration_count=16,
+            solver_velocity_iteration_count=1,
+            max_angular_velocity=1000.0,
+            max_linear_velocity=1000.0,
+            max_depenetration_velocity=5.0,
+            disable_gravity=False,
+        )
+
+        # Set each stacking cube deterministically
+        self.scene.cube_1 = RigidObjectCfg(
+            prim_path="{ENV_REGEX_NS}/Cube_1",
+            init_state=RigidObjectCfg.InitialStateCfg(pos=[0.4, 0.0, 0.0203], rot=[1, 0, 0, 0]),
+            spawn=UsdFileCfg(
+                usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Blocks/blue_block.usd",
+                scale=(1.0, 1.0, 1.0),
+                rigid_props=cube_properties,
+                semantic_tags=[("class", "cube_1")],
+            ),
+        )
         # make a smaller scene for play
         self.scene.num_envs = 50
         self.scene.env_spacing = 2.5