--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   scripts/tutorials/03_envs/run_cartpole_rl_env.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/h1_navigation_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/scripts/tutorials/03_envs/run_cartpole_rl_env.py b/scripts/tutorials/03_envs/run_cartpole_rl_env.py
index 3d4d0e53..30e42ed8 100644
--- a/scripts/tutorials/03_envs/run_cartpole_rl_env.py
+++ b/scripts/tutorials/03_envs/run_cartpole_rl_env.py
@@ -64,7 +64,8 @@ def main():
             # step the environment
             obs, rew, terminated, truncated, info = env.step(joint_efforts)
             # print current orientation of pole
-            print("[Env 0]: Pole joint: ", obs["policy"][0][1].item())
+            print("[Env 0]: Pole joint: ", obs["policy"][0][1].item(),
+                  "Reward: ", rew[0].item())
             # update counter
             count += 1
 
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
index 72f4bd82..21adad2a 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/h1/blind_navigation.py
@@ -15,9 +15,11 @@ from isaaclab.managers import ObservationTermCfg as ObsTerm
 from isaaclab.utils.noise import AdditiveUniformNoiseCfg as Unoise
 # source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py
 
-import isaaclab_tasks.manager_based.navigation.mdp as mdpa
+import isaaclab_tasks.manager_based.navigation.mdp as mdp
 import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp
-from isaaclab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg, RewardsCfg
+# from isaaclab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg, RewardsCfg
+from isaaclab_tasks.manager_based.locomotion.velocity.config.h1.h1_navigation_cfg import LocomotionVelocityRoughEnvCfg, RewardsCfg
+
 # from omni.isaac.lab.managers import TimedataCfg
 ##
 # Pre-defined configs
@@ -28,79 +30,79 @@ import math
 
 
 
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-        pose_command = ObsTerm(func=mdp.generated_commands, params={"command_name": "pose_command"})
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-            clip=(-1.0, 1.0),
-        )
+# @configclass
+# class ObservationsCfg:
+#     """Observation specifications for the MDP."""
+
+#     @configclass
+#     class PolicyCfg(ObsGroup):
+#         """Observations for policy group."""
+#         pose_command = ObsTerm(func=mdp.generated_commands, params={"command_name": "pose_command"})
+
+#         # observation terms (order preserved)
+#         base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
+#         base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
+#         projected_gravity = ObsTerm(
+#             func=mdp.projected_gravity,
+#             noise=Unoise(n_min=-0.05, n_max=0.05),
+#         )
+#         velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
+#         joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
+#         joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
+#         actions = ObsTerm(func=mdp.last_action)
+#         height_scan = ObsTerm(
+#             func=mdp.height_scan,
+#             params={"sensor_cfg": SceneEntityCfg("height_scanner")},
+#             noise=Unoise(n_min=-0.1, n_max=0.1),
+#             clip=(-1.0, 1.0),
+#         )
 
 #added code
-        time_left = ObsTerm(
-            func=lambda env: (
-                env.command_manager.get_term("pose_command").time_left / 
-                env.command_manager.get_term("pose_command").cfg.resampling_time_range[1]
-            ).unsqueeze(-1),
-            scale=1.0,
-            clip=(0.0, 1.0)
-        )
-
-        target_position = ObsTerm(
-            func=lambda env: (
-                (env.command_manager.get_term("pose_command").pos_command_w[:, :2] - 
-                env.scene["robot"].data.root_link_pos_w[:, :2]).view(-1, 2)  # Ensure shape (num_envs, 2)
-            ),
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-            scale=0.2,
-            clip=(-1.0, 1.0)
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
+        # time_left = ObsTerm(
+        #     func=lambda env: (
+        #         env.command_manager.get_term("pose_command").time_left / 
+        #         env.command_manager.get_term("pose_command").cfg.resampling_time_range[1]
+        #     ).unsqueeze(-1),
+        #     scale=1.0,
+        #     clip=(0.0, 1.0)
+        # )
+
+        # target_position = ObsTerm(
+        #     func=lambda env: (
+        #         (env.command_manager.get_term("pose_command").pos_command_w[:, :2] - 
+        #         env.scene["robot"].data.root_link_pos_w[:, :2]).view(-1, 2)  # Ensure shape (num_envs, 2)
+        #     ),
+        #     noise=Unoise(n_min=-0.1, n_max=0.1),
+        #     scale=0.2,
+        #     clip=(-1.0, 1.0)
+        # )
+
+    #     def __post_init__(self):
+    #         self.enable_corruption = True
+    #         self.concatenate_terms = True
+
+    # # observation groups
+    # policy: PolicyCfg = PolicyCfg()
 
 
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-
-
-    pose_command = mdp.UniformPose2dCommandCfg(
-        asset_name="robot",
-        simple_heading=False,
-        resampling_time_range=(8, 8),
-        debug_vis=True,
-        # Generate a tensor with [pos_x, pos_y, heading]
-        ranges=mdp.UniformPose2dCommandCfg.Ranges(
-            pos_x=(-3.0, 3.0),
-            pos_y=(-3.0, 3.0),
-            heading=(-math.pi, math.pi)
-        ),
-    )
+# @configclass
+# class CommandsCfg:
+#     """Command specifications for the MDP."""
+
+
+
+#     pose_command = mdp.UniformPose2dCommandCfg(
+#         asset_name="robot",
+#         simple_heading=False,
+#         resampling_time_range=(8, 8),
+#         debug_vis=True,
+#         # Generate a tensor with [pos_x, pos_y, heading]
+#         ranges=mdp.UniformPose2dCommandCfg.Ranges(
+#             pos_x=(-3.0, 3.0),
+#             pos_y=(-3.0, 3.0),
+#             heading=(-math.pi, math.pi)
+#         ),
+#     )
 
 
 @configclass
@@ -112,21 +114,21 @@ class H1Rewards(RewardsCfg):
 
 #code not being used now
 
-    # position_tracking = RewTerm(
-    #     func=mdpa.position_command_error_tanh,
-    #     weight=0.5,
-    #     params={"std": 4, "command_name": "pose_command"},
-    # )
-    # position_tracking_fine_grained = RewTerm(
-    #     func=mdpa.position_command_error_tanh,
-    #     weight=0.5,
-    #     params={"std": 0.4, "command_name": "pose_command"},
-    # )
-    # orientation_tracking = RewTerm(
-    #     func=mdpa.heading_command_error_abs,
-    #     weight=-0.2,
-    #     params={"command_name": "pose_command"},
-    # )
+    position_tracking = RewTerm(
+        func=mdp.position_command_error_tanh,
+        weight=0.5,
+        params={"std": 4, "command_name": "pose_command", "asset_cfg": SceneEntityCfg("robot", body_names=".*torso_link")},
+    )
+    position_tracking_fine_grained = RewTerm(
+        func=mdp.position_command_error_tanh,
+        weight=0.5,
+        params={"std": 0.4, "command_name": "pose_command", "asset_cfg": SceneEntityCfg("robot", body_names=".*torso_link")},
+    )
+    orientation_tracking = RewTerm(
+        func=mdp.heading_command_error_abs,
+        weight=-0.2,
+        params={"command_name": "pose_command"},
+    )
 
 
 #navigation related code from blind_3 paper
@@ -161,11 +163,6 @@ class H1Rewards(RewardsCfg):
         }
     )
 
-
-
-
-
-
     lin_vel_z_l2 = None
     track_lin_vel_xy_exp = RewTerm(
         func=mdp.track_lin_vel_xy_yaw_frame_exp,
@@ -283,630 +280,3 @@ class H1RoughEnvCfg_PLAY(H1RoughEnvCfg):
         self.events.base_external_force_torque = None
         self.events.push_robot = None
 
-
-
-
-
-
-
-
-
-#Velocity_env_cfg from Manager Based RL directory
-
-import math
-import torch
-from dataclasses import MISSING
-from isaaclab.utils.math import quat_rotate_inverse
-import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp
-import isaaclab.sim as sim_utils
-from isaaclab.assets import ArticulationCfg, AssetBaseCfg
-from isaaclab.envs import ManagerBasedRLEnvCfg
-from isaaclab.managers import CurriculumTermCfg as CurrTerm
-from isaaclab.managers import EventTermCfg as EventTerm
-from isaaclab.managers import ObservationGroupCfg as ObsGroup
-from isaaclab.managers import ObservationTermCfg as ObsTerm
-from isaaclab.managers import RewardTermCfg as RewTerm
-from isaaclab.managers import SceneEntityCfg
-from isaaclab.managers import TerminationTermCfg as DoneTerm
-from isaaclab.scene import InteractiveSceneCfg
-from isaaclab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from isaaclab.terrains import TerrainImporterCfg
-from isaaclab.utils import configclass
-from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from isaaclab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-# from omni.isaac.lab.terrains import TerrainImporterCfg  # Uncomment this line
-import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp
-from isaaclab_assets import H1_MINIMAL_CFG  # isort: skip
-from isaaclab.assets import RigidObjectCfg  # Add this import
-
-# from omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp import UniformVelocityCommand
-
-
-
-
-@configclass
-class MySceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = AssetBaseCfg(
-    prim_path="/World/ground",
-    spawn=sim_utils.UsdFileCfg(
-        usd_path="omniverse://localhost/NVIDIA/Assets/Isaac/4.5/Isaac/Environments/Simple_Warehouse/warehouse.usd"
-    )
-    )
-
-    # robots
-    robot: ArticulationCfg = MISSING
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=False,
-        mesh_prim_paths=["/World/ground"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
-    # lights
-    sky_light = AssetBaseCfg(
-        prim_path="/World/skyLight",
-        spawn=sim_utils.DomeLightCfg(
-            intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
-        ),
-    )
-
-
-
-
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(8, 8),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=False,
-        heading_control_stiffness=0.5,
-        debug_vis=False,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), 
-            lin_vel_y=(-1.0, 1.0), 
-            ang_vel_z=(-1.0, 1.0), 
-            heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-
-    pose_command = mdp.UniformPose2dCommandCfg(
-        asset_name="robot",
-        simple_heading=False,
-        resampling_time_range=(8, 8),
-        debug_vis=True,
-        # Generate a tensor with [pos_x, pos_y, heading]
-        ranges=mdp.UniformPose2dCommandCfg.Ranges(
-            pos_x=(-3.0, 3.0),
-            pos_y=(-3.0, 3.0),
-            heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-        pose_command = ObsTerm(func=mdp.generated_commands, params={"command_name": "pose_command"})
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-            clip=(-1.0, 1.0),
-        )
-
-#added code
-        time_left = ObsTerm(
-            func=lambda env: (
-                env.command_manager.get_term("pose_command").time_left / 
-                env.command_manager.get_term("pose_command").cfg.resampling_time_range[1]
-            ).unsqueeze(-1),
-            scale=1.0,
-            clip=(0.0, 1.0)
-        )
-
-        target_position = ObsTerm(
-            func=lambda env: (
-                (env.command_manager.get_term("pose_command").pos_command_w[:, :2] - 
-                env.scene["robot"].data.root_link_pos_w[:, :2]).view(-1, 2)  # Ensure shape (num_envs, 2)
-            ),
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-            scale=0.2,
-            clip=(-1.0, 1.0)
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "mass_distribution_params": (-5.0, 5.0),
-            "operation": "add",
-        },
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class RewardsCfg:
-    """Reward terms for the MDP."""
-
-    # -- task
-    # track_lin_vel_xy_exp = RewTerm(
-    #     func=mdp.track_lin_vel_xy_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    # )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_exp, weight=0.5, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    # )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-1.0e-5)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-7)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time,
-        weight=0.125,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*FOOT"),
-            "command_name": "base_velocity",
-            "threshold": 0.5,
-        },
-    )
-    undesired_contacts = RewTerm(
-        func=mdp.undesired_contacts,
-        weight=-1.0,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*THIGH"), "threshold": 1.0},
-    )
-    # -- optional penalties
-    flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=0.0)
-    dof_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=0.0)
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-
-
-# @configclass
-# class CurriculumCfg:
-#     """Curriculum terms for the MDP."""
-
-#     terrain_levels = CurrTerm(func=mdp.terrain_levels_vel)
-
-
-##
-# Environment configuration
-##
-
-
-@configclass
-class LocomotionVelocityRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: MySceneCfg = MySceneCfg(num_envs=4096, env_spacing=4)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    rewards: RewardsCfg = RewardsCfg()
-
-    commands: CommandsCfg = CommandsCfg()  # Make sure this is initialized
-
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    # curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 8
-        # simulation settings
-        self.sim.dt = 0.005
-
-        self.sim.render_interval = self.decimation
-        self.sim.disable_contact_processing = True
-        # self.sim.physics_material = self.scene.terrain.physics_material
-        self.sim.physx.gpu_max_rigid_patch_count = 10 * 2**15
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        if self.scene.height_scanner is not None:
-            self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-  
-
-
-
-
-
-
-
-
-
-#### All the rewards needed and not needed!
-
-"""Common functions that can be used to define rewards for the learning environment.
-
-The functions can be passed to the :class:`omni.isaac.lab.managers.RewardTermCfg` object to
-specify the reward function and its parameters.
-"""
-
-
-
-import torch
-from typing import TYPE_CHECKING
-
-from isaaclab.assets import RigidObject
-from isaaclab.managers import SceneEntityCfg
-from isaaclab.utils.math import combine_frame_transforms, quat_error_magnitude, quat_mul
-
-if TYPE_CHECKING:
-    from isaaclab.envs import ManagerBasedRLEnv
-
-
-import torch
-from typing import TYPE_CHECKING
-
-if TYPE_CHECKING:
-    from isaaclab.envs import ManagerBasedRLEnv
-
-import torch
-from typing import TYPE_CHECKING
-
-from isaaclab.managers import SceneEntityCfg
-from isaaclab.sensors import ContactSensor
-from isaaclab.utils.math import quat_rotate_inverse, yaw_quat
-
-if TYPE_CHECKING:
-    from isaaclab.envs import ManagerBasedRLEnv
-
-
-def feet_air_time(
-    env: ManagerBasedRLEnv, command_name: str, sensor_cfg: SceneEntityCfg, threshold: float
-) -> torch.Tensor:
-    """Reward long steps taken by the feet using L2-kernel.
-
-    This function rewards the agent for taking steps that are longer than a threshold. This helps ensure
-    that the robot lifts its feet off the ground and takes steps. The reward is computed as the sum of
-    the time for which the feet are in the air.
-
-    If the commands are small (i.e. the agent is not supposed to take a step), then the reward is zero.
-    """
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # compute the reward
-    first_contact = contact_sensor.compute_first_contact(env.step_dt)[:, sensor_cfg.body_ids]
-    last_air_time = contact_sensor.data.last_air_time[:, sensor_cfg.body_ids]
-    reward = torch.sum((last_air_time - threshold) * first_contact, dim=1)
-    # no reward for zero command
-    reward *= torch.norm(env.command_manager.get_command(command_name)[:, :2], dim=1) > 0.1
-    return reward
-
-
-def feet_air_time_positive_biped(env, command_name: str, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Reward long steps taken by the feet for bipeds.
-
-    This function rewards the agent for taking steps up to a specified threshold and also keep one foot at
-    a time in the air.
-
-    If the commands are small (i.e. the agent is not supposed to take a step), then the reward is zero.
-    """
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # compute the reward
-    air_time = contact_sensor.data.current_air_time[:, sensor_cfg.body_ids]
-    contact_time = contact_sensor.data.current_contact_time[:, sensor_cfg.body_ids]
-    in_contact = contact_time > 0.0
-    in_mode_time = torch.where(in_contact, contact_time, air_time)
-    single_stance = torch.sum(in_contact.int(), dim=1) == 1
-    reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
-    reward = torch.clamp(reward, max=threshold)
-    # no reward for zero command
-    reward *= torch.norm(env.command_manager.get_command(command_name)[:, :2], dim=1) > 0.1
-    return reward
-
-
-def feet_slide(env, sensor_cfg: SceneEntityCfg, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize feet sliding.
-
-    This function penalizes the agent for sliding its feet on the ground. The reward is computed as the
-    norm of the linear velocity of the feet multiplied by a binary contact sensor. This ensures that the
-    agent is penalized only when the feet are in contact with the ground.
-    """
-    # Penalize feet sliding
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    contacts = contact_sensor.data.net_forces_w_history[:, :, sensor_cfg.body_ids, :].norm(dim=-1).max(dim=1)[0] > 1.0
-    asset = env.scene[asset_cfg.name]
-
-    body_vel = asset.data.body_com_lin_vel_w[:, asset_cfg.body_ids, :2]
-    reward = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)
-    return reward
-
-
-def track_lin_vel_xy_yaw_frame_exp(
-    env, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of linear velocity commands (xy axes) in the gravity aligned robot frame using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset = env.scene[asset_cfg.name]
-    vel_yaw = quat_rotate_inverse(yaw_quat(asset.data.root_link_quat_w), asset.data.root_com_lin_vel_w[:, :3])
-    lin_vel_error = torch.sum(
-        torch.square(env.command_manager.get_command(command_name)[:, :2] - vel_yaw[:, :2]), dim=1
-    )
-    return torch.exp(-lin_vel_error / std**2)
-
-
-def track_ang_vel_z_world_exp(
-    env, command_name: str, std: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of angular velocity commands (yaw) in world frame using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset = env.scene[asset_cfg.name]
-    ang_vel_error = torch.square(
-        env.command_manager.get_command(command_name)[:, 2] - asset.data.root_com_ang_vel_w[:, 2]
-    )
-    return torch.exp(-ang_vel_error / std**2)
-
-
-
-
-
-def position_command_error_tanh(env: ManagerBasedRLEnv, std: float, command_name: str) -> torch.Tensor:
-    """Reward position tracking with tanh kernel."""
-    command = env.command_manager.get_command(command_name)
-    des_pos_b = command[:, :3]
-    distance = torch.norm(des_pos_b, dim=1)
-    return 1 - torch.tanh(distance / std)
-
-
-def heading_command_error_abs(env: ManagerBasedRLEnv, command_name: str) -> torch.Tensor:
-    """Penalize tracking orientation error."""
-    command = env.command_manager.get_command(command_name)
-    heading_b = command[:, 3]
-    return heading_b.abs()
-
-
-
-
-
-def position_command_error(env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize tracking of the position error using L2-norm.
-
-    The function computes the position error between the desired position (from the command) and the
-    current position of the asset's body (in world frame). The position error is computed as the L2-norm
-    of the difference between the desired and current positions.
-    """
-    # extract the asset (to enable type hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    command = env.command_manager.get_command(command_name)
-    # obtain the desired and current positions
-    des_pos_b = command[:, :3]
-    des_pos_w, _ = combine_frame_transforms(asset.data.root_state_w[:, :3], asset.data.root_state_w[:, 3:7], des_pos_b)
-    curr_pos_w = asset.data.body_state_w[:, asset_cfg.body_ids[0], :3]  # type: ignore
-    return torch.norm(curr_pos_w - des_pos_w, dim=1)
-
-
-def position_command_error_tanh(
-    env: ManagerBasedRLEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg
-) -> torch.Tensor:
-    """Reward tracking of the position using the tanh kernel.
-
-    The function computes the position error between the desired position (from the command) and the
-    current position of the asset's body (in world frame) and maps it with a tanh kernel.
-    """
-    # extract the asset (to enable type hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    command = env.command_manager.get_command(command_name)
-    # obtain the desired and current positions
-    des_pos_b = command[:, :3]
-    des_pos_w, _ = combine_frame_transforms(asset.data.root_state_w[:, :3], asset.data.root_state_w[:, 3:7], des_pos_b)
-    curr_pos_w = asset.data.body_state_w[:, asset_cfg.body_ids[0], :3]  # type: ignore
-    distance = torch.norm(curr_pos_w - des_pos_w, dim=1)
-    return 1 - torch.tanh(distance / std)
-
-
-def orientation_command_error(env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize tracking orientation error using shortest path.
-
-    The function computes the orientation error between the desired orientation (from the command) and the
-    current orientation of the asset's body (in world frame). The orientation error is computed as the shortest
-    path between the desired and current orientations.
-    """
-    # extract the asset (to enable type hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    command = env.command_manager.get_command(command_name)
-    # obtain the desired and current orientations
-    des_quat_b = command[:, 3:7]
-    des_quat_w = quat_mul(asset.data.root_state_w[:, 3:7], des_quat_b)
-    curr_quat_w = asset.data.body_state_w[:, asset_cfg.body_ids[0], 3:7]  # type: ignore
-    return quat_error_magnitude(curr_quat_w, des_quat_w)
-
-
-#newly added
-def compute_terminal_reward(env, Tr=1.0):
-
-    time_left = env.command_manager.get_term("pose_command").time_left  # e.g., [N, M]
-    active = (time_left < Tr).float()  # still [N, M]
-    
-    xb = env.scene["robot"].data.root_link_pos_w[:, :3]  # [N, 3]
-    target = env.command_manager.get_term("pose_command").pos_command_w[:, :3]  # [N, 3] or [N, M, 3]
-    
-    if target.dim() == 2:
-        target = target.unsqueeze(1)  # becomes [N, 1, 3]
-    xb = xb.unsqueeze(1)  # becomes [N, 1, 3]
-    
-    dist2 = torch.sum((xb - target)**2, dim=-1)
-    
-    reward = active * (1.0 / Tr) * (1.0 / (1.0 + dist2))
-    return reward.mean(dim=1)
-
-
-
-
-def compute_stalling_penalty(env):
-    """
-    Computes a penalty for stalling far from the target, matching paper's r_stall.
-    """
-    v = torch.norm(env.scene["robot"].data.root_com_lin_vel_w[:, :3], dim=1, keepdim=True)
-    
-    xb = env.scene["robot"].data.root_link_pos_w[:, :3].unsqueeze(1)  # [N, 1, 3]
-    target = env.command_manager.get_term("pose_command").pos_command_w[:, :3]
-    if target.dim() == 2:
-        target = target.unsqueeze(1)  # [N, 1, 3]
-    dist = torch.norm(xb - target, dim=-1)
-    
-    condition = ((v < 0.1) & (dist > 0.5)).float()  # [N, 1]
-    penalty = -1.0 * condition
-    return penalty.mean(dim=1)
-
-
-
-def compute_exploration_reward(env, task_reward_tracker=0.0, is_terminal=False, remove_threshold=0.5):
-
-    v = env.scene["robot"].data.root_com_lin_vel_w[:, :3]    # shape [N, 3]
-    x = env.scene["robot"].data.root_link_pos_w[:, :3]       # shape [N, 3]
-    x_star = env.command_manager.get_term("pose_command").pos_command_w[:, :3]  # shape [N, 3]
-    
-    if x_star.dim() == 3:  
-        x_star = x_star.squeeze(1)  # now shape [N, 3]
-    
-    dot_prod = torch.sum(v * (x_star - x), dim=1)  # shape [N]
-    norm_v = torch.norm(v, dim=1) + 1e-6           # shape [N]
-    norm_diff = torch.norm(x_star - x, dim=1) + 1e-6  # shape [N]
-    r_bias = dot_prod / (norm_v * norm_diff)        # shape [N]
-    
-
-    if isinstance(task_reward_tracker, (int, float)):
-        task_reward_tracker = torch.tensor(task_reward_tracker, device=v.device, dtype=torch.float32).repeat(v.shape[0])
-    mask = (task_reward_tracker < remove_threshold).float()  # shape [N]
-    
-    return r_bias * mask  # shape [N]
-
-
-def compute_stop_reward(env, dist_threshold=0.2, vel_threshold=0.05, stop_penalty=-10.0):
-
-    x = env.scene["robot"].data.root_link_pos_w[:, :3]  # [N, 3]
-    target = env.command_manager.get_term("pose_command").pos_command_w[:, :3]  # [N, 3]
-    dist = torch.norm(x - target, dim=1)  # [N]
-    
-    vel = torch.norm(env.scene["robot"].data.root_com_lin_vel_w[:, :3], dim=1)  # [N]
-    
-    penalty = stop_penalty * ((dist < dist_threshold).float() * (vel > vel_threshold).float())
-    return penalty  # [N]
\ No newline at end of file
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
index 1e0786f6..3d478a68 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/mdp/rewards.py
@@ -207,8 +207,6 @@ def compute_stalling_penalty(env):
     penalty = -1.0 * condition
     return penalty.mean(dim=1)
 
-
-
 def compute_exploration_reward(env, task_reward_tracker=0.0, is_terminal=False, remove_threshold=0.5):
 
     v = env.scene["robot"].data.root_com_lin_vel_w[:, :3]    # shape [N, 3]
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
index 0cdd7b53..0940b035 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py
@@ -67,16 +67,16 @@ class MySceneCfg(InteractiveSceneCfg):
     # robots
     robot: ArticulationCfg = MISSING
     
-    # cube = RigidObjectCfg(
-    #     prim_path="{ENV_REGEX_NS}/Cube",
-    #     spawn=UsdFileCfg(
-    #         usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Blocks/blue_block.usd",
-    #         scale=(3, 3, 3),
-    #         rigid_props= RigidBodyPropertiesCfg(
-    #             kinematic_enabled=False
-    #         )
-    #     )
-    # )
+    cube = RigidObjectCfg(
+        prim_path="{ENV_REGEX_NS}/Cube",
+        spawn=UsdFileCfg(
+            usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Blocks/blue_block.usd",
+            scale=(3, 3, 3),
+            rigid_props= RigidBodyPropertiesCfg(
+                kinematic_enabled=False
+            )
+        )
+    )
 
     # sensors
     height_scanner = RayCasterCfg(
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py
index b257bc19..0c407c77 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/navigation/config/anymal_c/navigation_env_cfg.py
@@ -32,7 +32,7 @@ class EventCfg:
     func=franka_stack_events.randomize_object_pose,
         mode="reset",
         params={
-            "pose_range": {"x": (0.4, 0.6), "y": (-0.10, 0.10), "z": (0.0203, 0.0203), "yaw": (-1.0, 1, 0)},
+            "pose_range": {"x": (-3, 3), "y": (-3, 3), "z": (0.0203, 0.0203), "yaw": (-1.0, 1, 0)},
             "min_separation": 0.1,
             "asset_cfgs": [SceneEntityCfg("cube")],
         },